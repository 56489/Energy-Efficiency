---
title: "CKME136 Capstone Project / Fall 2017"
author: 'Student: Walter Cios; Student ID 500791945'
output:
  word_document: default
  html_document: default
---


# Introduction

Building enclosures have become more energy efficient as energy codes have become more stringent in recent years, highlighting the importance of improvements such as better windows, orientation, or insulation. The heating and cooling load calculation is the first step of the HVAC design procedure. Heating and cooling load are used to right-size the HVAC equipment of a building. 

The scope of this project is to build a statistical model to predict the cooling load and to select the best performing model that provides the highest accuracy. The quantitative research will also investigate which variables are influencing the most the cooling efficiency and will explore the trade-offs between accuracy and interpretability of the model. Last, the energy efficiency impact when changing the building parameters(the window area or house orientation for example) will also be investigated as part of this study. 

This model and analysis will help HVAC equipment selection and prevent purchasing of oversized equipment.

Although the heating load is also present in the dataset, for the purpose of this project,time constrains, and similarities of calculation, I decided to investigate the cooling only and disregard the heating.

# Research Questions

-	Can the cooling load be predicted accurately with a statistical model?
-	Which variable are influencing the most the cooling efficiency 
-	Explore trade-offs between accuracy and interpretability of the model  
-	Investigate energy impact when changing some building parameters


# Dataset

The dataset analyzed is a public dataset, created by Angeliki Xifara & Athanasios
The data is a simulation of 768 residential seven person buildings, assumed to be in Athens, Greece and has eight input variables (surface area, wall area, roof area, orientation, glazing area.) and two output variables (heating and cooling load).


# Step 1: Data Cleaning and Preparation

- Load package readxl to read the raw data from Excel;
- Load package ggplot2 for graphs; 
- Load package dplyr for applying transformations on the dataframes; 
- Load package GGally to be used for plotting especially with function ggpairs
- Load package reshape2  to be used for transformation between long and wide formats
- Load package rpart to be used for partitioning and regression trees 
- Load package rpart.plot to be used to plot rpart trees
- Load package randomForestSRC that is the newer Random Forest package used for regression and classification
- Load package caret to be used for evaluation metrics

```{r message = FALSE, warning = FALSE}
library(readxl)
library(ggplot2)
library(dplyr)
library(GGally)
library(reshape2)
library(rpart)
library(rpart.plot)
library(randomForestSRC)
library(caret)
```

### Loading the data 

After examining the original Excel file, I noticed that only the first sheet has data and only this sheet will be read from the file.The other two empty sheets with greek names will be ignored 

```{r}
raw_data <- read_excel("./Data/ENB2012_data.xlsx", sheet = 1)
```

### Display structure of "raw_data"

The structure is reviewed to make sure that all 768 observations are loaded

```{r}
str(raw_data)
```

Because readxl loaded all variables as numeric there cannot be blank spaces.

### Review for Missing Data

The check for NA's can be done for the whole dataset

```{r}
sum(sapply(raw_data, is.na))

```

Or as an alternative, we could also have done an NA check per columns. As expected found none. 

```{r}
sapply(raw_data, function(x) { sum(is.na(x)) })
```
### Change column names to real variable names 

Review str for "data" and keep the raw_data in the workspace in case it's needed later

```{r}
data <- raw_data
names(data) <- c("Relative Compactness", "Surface Area", "Wall Area", "Roof Area", "Overall Height",
                 "Orientation", "Glazing Area", "Glazing Area Distribution", "Heating Load", "Cooling Load")

str(data)
```

### Exclude from analysis 48 tuples where "Glazing Area Distribution" = 0 

```{r}
unique(data$`Glazing Area Distribution`)
```
The original dataset has initially for "Glazing Area Distribution" six different values: "0" = No Glazing Area (no windows); 1 = Uniform (25 % windows on each side), 2 = North, 3 = East, 4 = South, 5 = West 

We see values like "0" (No Glazing Area) which never apply to residential buildings and might hinder us in answering our research questions by influencing the model fit, since there is not possible that a house has no windows and we will remove those "0" tuples from the Glazing Area Distribution 

By removing the 48 tuples with "0" Glazing Area Distribution we are removing also all the "0" in "Glazing Area".


```{r}
data <- filter(data, `Glazing Area Distribution` != 0)
unique(data$`Glazing Area Distribution`)
```
Now Glazing Area Distribution has only 5 values and "0" has been removed 

### Converting categorical variables to factors

As per the dataset description, I established that there are two categorical variables:"Orientation" and "Glazing Area Distribution". The variable "Orientation" has only four values, the cardinal directions: N,W,S,E.
The variable "Glazing Area Distribution" has all the above four and in addition "1" for "Uniform".

Using the data dictionary from the authors we created mappings from the integers in the raw data to categorical values.

"Relevel" was used in order to bring "Uniform" (that has value "1") to the first position

```{r}
toCategorical <- Vectorize(function(column) {
  switch(as.character(column),
                      "1" = "Uniform",
                      "2" = "North",
                      "3" = "East",
                      "4" = "South",
                      "5" = "West",
                      NA)
})

data$`Glazing Area Distribution` <- as.factor(toCategorical(data$`Glazing Area Distribution`))
data$Orientation <- as.factor(toCategorical(data$Orientation))

data$`Glazing Area Distribution` <- relevel(data$`Glazing Area Distribution`, "Uniform")
```

### Review summary for "data"

```{r}
summary(data)
```

From the summmary we can see that Orientation and Glazing Area Distribution, are qualitative-categorical variables, the other variables are quantitative-continuous. Those conclusions and the Min and Max will be used in an Excel table that explains the dataset in more detail.

### Counting the unique values per variable

```{r}
sapply(data, function(col) length(unique(col)))
```

Review str after the change to display the 720 remaining values

```{r}
str(data)
```

# Step 2: Exploratory Data Analysis

Split up the categorical variables from the continuous ones, into a new dataframe "data_categorical" and keep the continuos variables in "data_numeric".

```{r}
data_numeric <- select(data, -`Orientation`, -`Glazing Area Distribution`)
data_categorical <- select(data, `Orientation`, `Glazing Area Distribution`)
```

### Checking for correlated variables using paired scatterplots for "data_numeric"

Next I visually checked for correlated variables in the numerical dataset using an optimal display of plot and selected the package "GGally" where the function ggpairs is being used for creating scatterplots between all pairs of continuous variables from "data_numeric".


```{r fig.height=10, fig.width=10}
ggpairs(data_numeric)
```


The following are the interpretations of the major correlations: 

Relative Compacteness & Surface Area / strong almost perfect negative correlation at -0.992

Relative Compacteness & Wall Area / low negative correlation

Relative Compacteness & Roof Area / strong negative correlation

Surface Area & Overall Height / strong negative correlation

Surface Area & Roof Area / strong positive correlation 

Relative Compactness & Glazing Area / "0" correlation

Relative Compactness & Heating Load /medium strong positive corelation

Wall Area & Heating Load / low postitive correlation

Relative Compacteness & Cooling Load / poitive high correlation 

Surface Area & Cooling Load / negative high correlation 

Wall Area & Cooling Load / low positive correlation

Roof Area & Cooling Load / strong negative correlation

Overall Height & Cooling Load / strong positive correlation

Glazing Area & Cooling Load / low positive correlation 

Heating Load & Cooling Load / strong almost perfect positive correlation 


### Outlier detection

Review if there are outliers by using boxplots. Going forward I will try to use only ggplot for all graphs to create an consistent view

Looking at the Excel Table "Variable Review for Analysis" that has the Min and Max, I decided to split the boxplots of data-numeric into three categories by cluster of data: small (values 0-1); medium (values >1-50); large (>50)

```{r}
small <- select(data_numeric,`Relative Compactness`,`Glazing Area`)

medium <- select(data_numeric,`Overall Height`,`Heating Load`,`Cooling Load`)

large <- select(data_numeric,`Surface Area`,`Roof Area`,`Wall Area`)
```

#### Creating boxplots for the grouped variables into small, medium and large

I used a couple of features to make the boxplots better visible on paper:

-	The boxplots were plotted using ggplot2; 
-	In the geom_boxplot I filled the inside "green"; 
-	For the median, I used "lwd" = 1.2, slightly more than normal, ("lwd" stands for line width) to display a thicker line and this way to be better visible;
-	Used geom_jitter to add a small amount of variation and make the dots more visible by changing: the shape to be "diamond" = shape # 18, colour "blue" to show better on green, and increase slightly the width of the blue diamond dots

We are splitting the numeric data in 3 data frames because the variables have different scales.

We also used melt from the reshape2 package because the boxplots have the variable name on the x-axis and the long data format(one column with the variable name and another one with the value) can be easily used by ggplot2 to create the side-by-side boxplots.

```{r message=F}
small_long <- melt(small)
ggplot(data=small_long, aes(x = variable, y = value)) +
  geom_boxplot(fill = "green", lwd = 1.2) +
  geom_jitter(height = 0, width = 0.4, shape = 18, color = "blue") +
  ggtitle("Boxplots of variables with small values") +
  xlab("Variable Name") +
  ylab("Values")
```

The boxplots of variables with small values are showing that we have no outliers and that there are several data points that have the same values what is the result of the simulated values in the dataset. "Relative Compacteness" takes 12 values only and "Glazing Area " only four.

```{r message=F}
medium_long <- melt(medium)

ggplot(data=medium_long, aes(x = variable, y = value)) +
  geom_boxplot(fill = "green",lwd = 1.2) +
  geom_jitter(height = 0, width = 0.4, shape = 18, color = "blue") +
  ggtitle("Boxplots of variables with medium values") +
  xlab("Variable Name") +
  ylab("Values")
```

The boxplots of variables with medium values are showing that we have no outliers and that there are several data points clustered in the bottom part of the boxplot of the Heating Load and Cooling Load (between 10 and 15).

For the Overall Height, this has only two values 3,5 and 7. 

For Heating Load we see that the data is mostly clustered in the first quartile of the boxplot.

For Cooling Load we see more variabilitry in the data but most data is grouped together in the first quartile.


```{r message=F}
large_long <- melt(large)

ggplot(data=large_long, aes(x = variable, y = value)) +
  geom_boxplot(fill = "green", lwd = 1.2) +
  geom_jitter(height = 0, width = 0.4, shape = 18, color = "blue") +
  ggtitle("Boxplots of variables with large values") +
  xlab("Variable Name") +
  ylab("Values")
```

The boxplots of variables with high values are showing that we have no outliers and that Surface Area has only 12 possible values, Wall Area only 7 values and Roof Area only 4 values. 



### Creating a Correlation Matrix 

I created a Correlation Matrix Heatmap to further display the correlation of the continuous variables

I used a couple of features to make the Correlation Matrix Heatmap a better visualization:

-	The cor method "Pearson"
-	geom_text to write the cor values as text in the square 
-	Selected 2 colours "green3" and "magenta 3" 
From: http://sape.inf.usi.ch/quick-reference/ggplot2/colour
-	I obtained a scale gradient of colours with values between -1 and 1
-	The green colour is showing the negative correlation and the magenta colour the positive correlation
-	The geom_tile is displaying the rectangles 
-	Set the figure height at 8 and width at 10 to better show up when printed.

Having the Correlation Matrix Heatmap it is easy to visualize the lighter and darker colours that stand for low and high correlation. 

```{r fig.height=8, fig.width=10}
correlationMatrix <- cor(data_numeric, method = "pearson")
correlationsLong <- melt(correlationMatrix)

ggplot(data = correlationsLong, aes(x = Var1, y = Var2)) +
  geom_tile(aes(fill = value)) +
  geom_text(aes(label = round(value, 2))) +
  scale_fill_gradient(low = "green3", high = "magenta3") +
  ggtitle("Correlation Matrix Heatmap") +
  xlab("Variable Names") +
  ylab("Variable Names") 
```

### Investigating the Categorical Variables

"Orientation" that has only the four values of the cardinal directions: East, North, South, West

```{r}
table(data_categorical$Orientation)
```

The tables are displayed for both categorical variable "Orientation" and "Glazing Are Distribution"to show that the  "Glazing Area Distribution" has the four values of the cardinal directions and includes also"Uniform" 

```{r}
table(data_categorical$`Glazing Area Distribution`)
```

Next we will use facet_grid to plot the histograms of the Cooling Load for each value of the categorical variables.

```{r}
plotData <- select(data, `Cooling Load`, `Orientation`)

ggplot(plotData, aes(x = `Cooling Load`, fill = `Orientation`)) +
  geom_histogram(binwidth = 2.5) +
  facet_grid(~ Orientation) +
  ggtitle("Cooling Load by Orientation")
```

Plot the categorical variable "Orientation" agains the predicted variable "Cooling Load"

For all orientations the cooling load distribution is roughly bimodal, with one peak around 15 and the other around 35.


```{r fig.height=5, fig.width=9}
plotData <- select(data, `Cooling Load`, `Glazing Area Distribution`)

ggplot(plotData, aes(x = `Cooling Load`, fill = `Glazing Area Distribution`)) +
  geom_histogram(binwidth = 2.5) +
  facet_grid(~ `Glazing Area Distribution`) +
  ggtitle("Cooling Load by Glazing Area Distribution")
```

Similar results for Glazing Area Distribution

The initial intent was to use One-way ANOVA to check if these categorical variables would have an impact on the predicted variable, but because the distributions within the groups are clearly not normal, we cannot do this.

# Step 3 Define Model Evaluation Procedure 

The most important criterion for choosing the best model will be R2. For linear models, we will not consider 
adjusted R2, but we will try to use information criteria (AIC and/or BIC). For non-linear models like CART and Random Forest, we will use k-fold cross-validation (k = 10) to pick the best performing model according to R2. We will also use MSE and MAE as evaluation measures because they were used by the authors of the original study.

We will also consider the interpretability of the models and how can they help us to answer the original research questions.

From the two output variables we are predicting the "Cooling Load" only (that was marked in the original study with "y2"), and this is why we exclude from the data the "Heating Load" from the modelData. 

We will use set.seed to ensure the reproducibility of the results. The data is shuffled because it is generated and the order in which it was generated would create biased folds in cross-validation.

Our first priority is to replicate the original results, so we start with random forest and 10-fold cross-validation. We will also use a classical train-test split(70-30) for other models.

The following code block contains the configuration of the model evaluation procedures, so we can easily change k or the training proportion if needed.

```{r}
modelData <- select(data, -`Heating Load`)
set.seed(100)
shuffledModelData <- modelData[sample(1:nrow(modelData), nrow(modelData), replace = F),]
k <- 10
trainProportion <- 0.7
```

### Short Definitions of Evaluation Measures

R-squared is the percentage of the response variable variation that is explained by a model.
R-squared is always between 0 and 100%: 0% indicates that the model explains none of the variability of the response data around its mean; and 100% indicates that the model explains all the variability of the response data around its mean. In general we can say that the higher the R-squared, the better the model fits the data. 

The Mean Absolute Error (MAE) and Root mean squared error (RMSE) are two of the most common metrics to measure accuracy for continuous variables. The Mean Absolute Error (MAE) measures the average magnitude of the errors in a set of predictions, without considering their direction. It's the average over the test sample of the absolute differences between prediction and actual observation where all differences have equal weight.

The Root Mean Squared Error (RMSE) is a quadratic scoring rule that also measure the average magnitude of error. It's the square root of the average of squared differences between prediction and actual observation.


# Step 4 Models

We will start with random forest with 10-fold cross-validation because this is what the authors of the original study used and we will try to replicate their results first.

Steps 5 and 6 will de done individually for each model.

# Step 4.3 Random Forest

The authors didn't mention which parameters they used for random forest, so we will do a grid search and use the cross-validation results to pick the best performing model. We will use a simple grid search for the hyperparameter selection. In the grid we picked values such that the best performing model is not at the extremes. The intervals were picked after several trial runs.

```{r}
validateRF <- function(model, data) {
  observed <- data$`Cooling Load`
  predicted <- predict(model, newdata = data)$predicted
  list(
    R2 = R2(predicted, observed),
    MSE = RMSE(predicted, observed)^2,
    MAE = MAE(predicted, observed))
}

RFparams <- expand.grid(
  mtry = 2:4,
  nodesize = seq(5, 15, 5),
  ntree = seq(50, 150, 50),
  nodedepth = 2:7)
```

For evaluation we will compute several measures (R2, MSE, MAE) and we will take the mean over all folds. Because we are fitting 162 models, we will only display the best 10 and the worst 5, sorted by R2 in descending order.

```{r}
RFevalCV <- data.frame()
for (paramIndex in 1:nrow(RFparams)) {
  params <- RFparams[paramIndex,]
  
  folds <- cut(1:nrow(shuffledModelData), breaks=k, label=F)
  
  cvR2 <- c()
  cvMSE <- c()
  cvMAE <- c()
  for (foldIndex in 1:k) {
    testData <- modelData[folds == foldIndex,]
    trainData <- modelData[folds != foldIndex,]
    
    model <- rfsrc(`Cooling Load` ~ ., data = as.data.frame(trainData),
                  mtry = params$mtry,
                  nodesize = params$nodesize,
                  ntree = params$ntree,
                  nodedepth = params$nodedepth)
    modelEval <- validateRF(model, as.data.frame(testData))
    cvR2 <- c(cvR2, modelEval$R2)
    cvMSE <- c(cvMSE, modelEval$MSE)
    cvMAE <- c(cvMAE, modelEval$MAE)
  }
  
  evalRow <- mutate(params, R2 = mean(cvR2), MSE = mean(cvMSE), MAE = mean(cvMAE))
  RFevalCV <- rbind(RFevalCV, evalRow)
}
RFevalCV <- arrange(RFevalCV, desc(R2))
rbind(head(RFevalCV, 10), tail(RFevalCV, 5))
```

We will now fit the model that performed best in cross-validation on the whole dataset.

```{r}
RFmodel <- rfsrc(`Cooling Load` ~ ., data = as.data.frame(modelData), importance = T,
                  mtry = RFevalCV$mtry[1],
                  nodesize = RFevalCV$nodesize[1],
                  ntree = RFevalCV$ntree[1],
                  nodedepth = RFevalCV$nodedepth[1])
```

# Step 4.2 CART

For CART we will use a train-test split, but we will still use the grid search method for hyperparameter selection. In the grid we picked values such that the best performing model is not at the extremes. The intervals were picked after several trial runs.

```{r}
validateCART <- function(model, data) {
  observed <- data$`Cooling Load`
  predicted <- predict(model, newdata = data)
  list(
    R2 = R2(predicted, observed),
    MSE = RMSE(predicted, observed)^2,
    MAE = MAE(predicted, observed))
}

CARTparams <- expand.grid(
  minsplit = seq(10, 40, 10),
  cp = c(1e-5, 0.0005, 0.0001, 0.005, 0.001),
  maxdepth = 4:8)
```

We will display the CART model performance similar to what we did for random forest.

```{r}
CARTeval <- data.frame()
for (paramIndex in 1:nrow(CARTparams)) {
  params <- CARTparams[paramIndex,]
  
  inTrain <- 1:nrow(modelData) < trainProportion * nrow(modelData)
  
  testData <- modelData[!inTrain,]
  trainData <- modelData[inTrain,]
  
  model <-  rpart(`Cooling Load` ~ ., data = trainData, method = "anova",
                  control = rpart.control(
                    minsplit = params$minsplit,
                    maxdepth = params$maxdepth,
                    cp = params$cp))
  modelEval <- validateCART(model, testData)
  
  evalRow <- mutate(params, R2 = modelEval$R2, MSE = modelEval$MSE, MAE = modelEval$MAE)
  CARTeval <- rbind(CARTeval, evalRow)
}
CARTeval <- arrange(CARTeval, desc(R2))
rbind(head(CARTeval, 10), tail(CARTeval, 5))
```

We now fit the CART model on the whole dataset.

```{r}
CARTmodel <- rpart(`Cooling Load` ~ ., data = modelData, method = "anova",
                   control = rpart.control(
                     minsplit = CARTeval$minsplit[1],
                     maxdepth = CARTeval$maxdepth[1],
                     cp = CARTeval$cp[1]))
```

### Step 4.1 Linear Model

We will try to fit a linear model on the data, but more investigation is needed on how to handle the nonlinear relationships(Relative Compactness vs Cooling Load for example).

### Step 4.4 Gradient Boosted Trees

In general, boosting is used for weak learners and our decision tree was already a very good fit on the data with a R2 of over 0.95, and this is why we will not pursue this matter further.

### Model Comparison

We will now compare the results of all the models that we have trained until now.

```{r}
evaluationResults <- data.frame(
  model = c("original study", "Random Forest", "CART"),
  R2 = c(NA, RFevalCV$R2[1], CARTeval$R2[1]),
  MSE = c(6.59, RFevalCV$MSE[1], CARTeval$MSE[1]),
  MAE = c(1.42, RFevalCV$MAE[1], CARTeval$MAE[1]))

evaluationResults
```

As we see from that table, the random forest with our chosen parameters seems to outperform the model from the original study, however there are some other facts that contribute to this difference:

- We removed the 48 building with "No Glazing Area"; we experimented with models that included them and the errors on these buildings were the largest and our concern was that they biased the models and we couldn't answer the research questions effectively
- They repeated the cross-validation many times and the standard error for their MSE was 1.6, which puts us roughly at 2 standard errors from their average MSE
- We used a newer random forest package that didn't exist when they did their study(2011)

### The best performing model

Both models: Random Forest and CART have very similar R-squared (Random Forest has 0.9689 nd CART has 0.9722)

The best performing Model from the Initial Results seems to be Random Forest since it has a lower MSE at 2.79 compared to CART that has 5.59



### A more in-depth look

Although the CART MSE is almost twice the random forest MSE, the CART R2 is surprisingly larger, so we want to have a more in-depth look at the predictions.

```{r}
predicted <- predict(RFmodel)$predicted
observed <- modelData$`Cooling Load`
plot(observed, predicted)
abline(0, 1)
```


```{r}
predicted <- predict(CARTmodel)
observed <- modelData$`Cooling Load`
plot(observed, predicted)
abline(0, 1)
```

After the Inintial Results submission, I will investigate the predictions with the largest errors and try to identify if there is a pattern or some easily identifiable conditions when the model makes mistakes.

For both models Random Forest and CART I selected not to investigate confidence intervals because there is no standard theoretically sound way to compute them like there is for linear models.

# Step 7: Answering the Research Questions

### Can the cooling load be predicted accurately with a statistical model?

As a result of the initial analysis, yes this can be done byusing the Random Forest statistical model. 


### Which are the most infuential variables when predicting the cooling efficiency?

We will try to use all the models to answer this question and then compare the results:

### CART

```{r fig.width=12, fig.height=6}
rpart.plot(CARTmodel, uniform = T)
```

When we look at the tree it seems that Relative Compactness, Wall Area and Glazing Area are the most important variables, as they make up the first three levels. We can see the Surface Area and Glazing Area Distribution show up a few times on the last 2 levels. We don't see at all the Roof Area, Orientation and Overall Height.

### Random Forest

vimp is a function from the randomForestSRC package that computes the variable importance, similar to the coefficient p-values in linear models.

```{r fig.width=9, fig.height=6}
plot(vimp(RFmodel))
```

As we can see from the variable importance plot, the Random Forest model gives different results. THe most important variable is the Roof Area, followed closely by the Surface Area, while the Overall Height and Relative Compactness have an average importance. Wall Area and Glazing area have minor influence on the predictions, while Orientation and Glazing Area Distribution don't seem to matter for this model.